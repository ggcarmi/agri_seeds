{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agricultural Analytics Demo\n",
    "\n",
    "This notebook demonstrates the complete workflow of our agricultural analytics system:\n",
    "1. PDF Text and Table Extraction\n",
    "2. Futures Prices EDA and Anomaly Detection\n",
    "3. Report Summarization using LLM\n",
    "\n",
    "Let's start by importing the required libraries and initializing our components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from agri_analytics import AgriAnalytics\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PDF Processing\n",
    "\n",
    "First, we'll extract text and tables from the Oilseeds Outlook Report using multiple extraction methods including PDF parsing, tabula-py for tables, and OCR for complex layouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the analytics system\n",
    "analytics = AgriAnalytics()\n",
    "\n",
    "# Extract PDF content\n",
    "pdf_text, pdf_tables, cv_tables = analytics.extract_pdf_content('Oilseeds Outlook Report 122024.pdf')\n",
    "\n",
    "print(f\"Number of tables extracted: {len(pdf_tables)}\\n\")\n",
    "print(\"First 500 characters of extracted text:\")\n",
    "print(pdf_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the extracted tables. The system uses both tabula-py for structured tables and computer vision techniques for complex layouts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display extracted tables\n",
    "print(\"Tables extracted using tabula-py:\")\n",
    "for i, table in enumerate(pdf_tables):\n",
    "    print(f\"\\nTable {i+1}:\")\n",
    "    display(table)\n",
    "\n",
    "print(\"\\nTables detected using CV:\")\n",
    "for i, table in enumerate(cv_tables):\n",
    "    print(f\"\\nCV-detected Table {i+1}:\")\n",
    "    # Convert OCR output to DataFrame for better visualization\n",
    "    text_data = [word for word in table['text'] if word.strip() != '']\n",
    "    print(' '.join(text_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Futures Prices Analysis\n",
    "\n",
    "Now let's analyze the futures prices data. We'll look at price trends for different commodities and detect anomalies using Isolation Forest algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load futures data\n",
    "futures_data = analytics.load_futures_data('futures_prices.csv')\n",
    "\n",
    "# Basic statistics\n",
    "print(\"Basic statistics for futures prices:\")\n",
    "display(futures_data.groupby('Symbol')['Close'].describe())\n",
    "\n",
    "# Plot price trends by commodity\n",
    "plt.figure(figsize=(15, 8))\n",
    "for symbol in futures_data['Symbol'].unique():\n",
    "    symbol_data = futures_data[futures_data['Symbol'] == symbol]\n",
    "    plt.plot(symbol_data['Date'], symbol_data['Close'], label=symbol, alpha=0.7)\n",
    "\n",
    "plt.title('Futures Prices Over Time by Commodity')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate volatility\n",
    "volatility = futures_data.groupby('Symbol')['Close'].agg(['std', 'mean']).round(2)\n",
    "volatility['cv'] = (volatility['std'] / volatility['mean'] * 100).round(2)\n",
    "print(\"\\nPrice Volatility by Commodity (Coefficient of Variation):\")\n",
    "display(volatility.sort_values('cv', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly Detection\n",
    "\n",
    "Let's detect and visualize price anomalies using Isolation Forest. We'll set a contamination factor of 0.1 (expecting about 10% of the points to be anomalies) and use 150 estimators for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies\n",
    "anomalies, stats = analytics.detect_anomalies(contamination=0.1, n_estimators=150)\n",
    "\n",
    "print(\"Anomaly Detection Statistics:\")\n",
    "print(json.dumps(stats, indent=2))\n",
    "\n",
    "# Visualize anomalies\n",
    "analytics.visualize_prices_and_anomalies()\n",
    "\n",
    "# Additional analysis of anomalies\n",
    "anomaly_dates = futures_data[futures_data['is_anomaly'] == -1][['Date', 'Symbol', 'Close']]\n",
    "print(\"\\nTop 10 Anomalous Price Points:\")\n",
    "display(anomaly_dates.sort_values('Close', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Report Summarization\n",
    "\n",
    "Finally, let's use our LLM-based summarizer to generate a structured summary of the report. The summarizer uses a Llama-2 model to analyze the text and extract key insights in four main categories:\n",
    "- Market Trends\n",
    "- Price Forecasts\n",
    "- Supply/Demand Analysis\n",
    "- Key Risk Factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate report summary\n",
    "summary_json = analytics.generate_report_summary()\n",
    "summary = json.loads(summary_json)\n",
    "\n",
    "print(\"Report Summary:\")\n",
    "print(\"=============\\n\")\n",
    "\n",
    "for section, content in summary['summary'].items():\n",
    "    print(f\"{section.replace('_', ' ').title()}:\")\n",
    "    print(f\"{content}\\n\")\n",
    "\n",
    "print(\"Key Metrics:\")\n",
    "print(json.dumps(summary['key_metrics'], indent=2))\n",
    "\n",
    "print(\"\\nExtracted Data Statistics:\")\n",
    "print(f\"Number of tables found: {summary['extracted_tables_count']}\")\n",
    "print(f\"Number of CV-detected tables: {summary['cv_detected_tables_count']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook has demonstrated the complete workflow of our agricultural analytics system:\n",
    "1. We extracted text and tables from the PDF report using multiple methods (PDF parsing, tabula-py, and OCR)\n",
    "2. We analyzed futures prices data, visualized trends, and detected anomalies using Isolation Forest\n",
    "3. We generated a structured summary of the report using our LLM-based summarizer\n",
    "\n",
    "The system successfully combines traditional data analysis with modern ML techniques to provide comprehensive insights into agricultural markets."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
